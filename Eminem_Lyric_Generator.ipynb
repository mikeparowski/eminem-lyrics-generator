{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/miniconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import random, sys, io, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('data/eminem-songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_lines(df):\n",
    "    words = []\n",
    "    \n",
    "    for index, row in df['lyrics'].iteritems():\n",
    "        row = str(row).lower()\n",
    "        for line in row.split('|-|'):\n",
    "            new_words = re.findall(r\"\\b[a-z']+\\b\", unidecode(line))\n",
    "            words = words + new_words\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyric_lines = get_tokenized_lines(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 377669\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 50 + 1\n",
    "sequences = []\n",
    "\n",
    "for i in range(SEQ_LENGTH, len(all_lyric_lines)):\n",
    "    seq = all_lyric_lines[i - SEQ_LENGTH: i]\n",
    "    sequences.append(seq)\n",
    "\n",
    "print(\"Total Sequences: %d\" % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        data.append(\" \".join(line))\n",
    "        \n",
    "    file = open(filename, 'w')\n",
    "    file.write('\\n'.join(data))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = \"sequences.txt\"\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 17495\n"
     ]
    }
   ],
   "source": [
    "vocab = set(all_lyric_lines)\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i: w for w, i in word_to_index.items()}\n",
    "word_indices = [word_to_index[word] for word in vocab]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"vocabulary size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_lines(lines, seq_len):\n",
    "    tokenized = np.zeros((len(lines), seq_len))\n",
    "    \n",
    "    for r, line in enumerate(lines):\n",
    "        for c, word in enumerate(line):\n",
    "            tokenized[r, c] = word_to_index[word]\n",
    "            \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_seq = get_tokenized_lines(sequences, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377669,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_seq[:, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X, y = tokenized_seq[:, :-1], tokenized_seq[:, -1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape (377669, 50)\n",
      "y_shape (377669, 17495)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_shape\", X.shape)\n",
    "print(\"y_shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            874750    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 17495)             1766995   \n",
      "=================================================================\n",
      "Total params: 2,792,645\n",
      "Trainable params: 2,792,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "377669/377669 [==============================] - 1686s 4ms/step - loss: 6.6358 - acc: 0.0409\n",
      "Epoch 2/50\n",
      "377669/377669 [==============================] - 1687s 4ms/step - loss: 6.1965 - acc: 0.0650\n",
      "Epoch 3/50\n",
      "377669/377669 [==============================] - 1803s 5ms/step - loss: 5.9854 - acc: 0.0809\n",
      "Epoch 4/50\n",
      "377669/377669 [==============================] - 1786s 5ms/step - loss: 5.7943 - acc: 0.0930\n",
      "Epoch 5/50\n",
      "377669/377669 [==============================] - 1833s 5ms/step - loss: 5.6363 - acc: 0.1046\n",
      "Epoch 6/50\n",
      "377669/377669 [==============================] - 1777s 5ms/step - loss: 5.4952 - acc: 0.1147\n",
      "Epoch 7/50\n",
      "377669/377669 [==============================] - 1816s 5ms/step - loss: 5.3650 - acc: 0.1228\n",
      "Epoch 8/50\n",
      "377669/377669 [==============================] - 1914s 5ms/step - loss: 5.2462 - acc: 0.1309\n",
      "Epoch 9/50\n",
      "377669/377669 [==============================] - 2054s 5ms/step - loss: 5.1360 - acc: 0.1380\n",
      "Epoch 10/50\n",
      "377669/377669 [==============================] - 2121s 6ms/step - loss: 5.0336 - acc: 0.1448\n",
      "Epoch 11/50\n",
      "377669/377669 [==============================] - 2117s 6ms/step - loss: 4.9407 - acc: 0.1518\n",
      "Epoch 12/50\n",
      "377669/377669 [==============================] - 2130s 6ms/step - loss: 4.8534 - acc: 0.1586\n",
      "Epoch 13/50\n",
      "377669/377669 [==============================] - 1974s 5ms/step - loss: 4.7728 - acc: 0.1653\n",
      "Epoch 14/50\n",
      "377669/377669 [==============================] - 1988s 5ms/step - loss: 4.6964 - acc: 0.1725\n",
      "Epoch 15/50\n",
      "377669/377669 [==============================] - 1977s 5ms/step - loss: 4.6252 - acc: 0.1793\n",
      "Epoch 16/50\n",
      "377669/377669 [==============================] - 1930s 5ms/step - loss: 4.5576 - acc: 0.1862\n",
      "Epoch 17/50\n",
      "377669/377669 [==============================] - 1824s 5ms/step - loss: 4.4931 - acc: 0.1931\n",
      "Epoch 18/50\n",
      "377669/377669 [==============================] - 1842s 5ms/step - loss: 4.4338 - acc: 0.1999\n",
      "Epoch 19/50\n",
      "377669/377669 [==============================] - 1849s 5ms/step - loss: 4.3761 - acc: 0.2064\n",
      "Epoch 20/50\n",
      "377669/377669 [==============================] - 1857s 5ms/step - loss: 4.3227 - acc: 0.2126\n",
      "Epoch 21/50\n",
      "377669/377669 [==============================] - 1863s 5ms/step - loss: 4.2676 - acc: 0.2195\n",
      "Epoch 22/50\n",
      "377669/377669 [==============================] - 1878s 5ms/step - loss: 4.2194 - acc: 0.2253\n",
      "Epoch 23/50\n",
      "377669/377669 [==============================] - 1888s 5ms/step - loss: 4.1700 - acc: 0.2316\n",
      "Epoch 24/50\n",
      "377669/377669 [==============================] - 1903s 5ms/step - loss: 4.1238 - acc: 0.2377\n",
      "Epoch 25/50\n",
      "377669/377669 [==============================] - 1916s 5ms/step - loss: 4.0786 - acc: 0.2442\n",
      "Epoch 26/50\n",
      "377669/377669 [==============================] - 1927s 5ms/step - loss: 4.0372 - acc: 0.2493\n",
      "Epoch 27/50\n",
      "377669/377669 [==============================] - 1938s 5ms/step - loss: 3.9962 - acc: 0.2550\n",
      "Epoch 28/50\n",
      "377669/377669 [==============================] - 1948s 5ms/step - loss: 3.9547 - acc: 0.2606\n",
      "Epoch 29/50\n",
      "377669/377669 [==============================] - 1960s 5ms/step - loss: 3.9173 - acc: 0.2655\n",
      "Epoch 30/50\n",
      "377669/377669 [==============================] - 1963s 5ms/step - loss: 3.8779 - acc: 0.2713\n",
      "Epoch 31/50\n",
      "377669/377669 [==============================] - 1977s 5ms/step - loss: 3.8432 - acc: 0.2771\n",
      "Epoch 32/50\n",
      "377669/377669 [==============================] - 1979s 5ms/step - loss: 3.8086 - acc: 0.2816\n",
      "Epoch 33/50\n",
      "377669/377669 [==============================] - 1985s 5ms/step - loss: 3.7747 - acc: 0.2869\n",
      "Epoch 34/50\n",
      "377669/377669 [==============================] - 1990s 5ms/step - loss: 3.7421 - acc: 0.2914\n",
      "Epoch 35/50\n",
      "377669/377669 [==============================] - 2033s 5ms/step - loss: 3.7112 - acc: 0.2964\n",
      "Epoch 36/50\n",
      "377669/377669 [==============================] - 2035s 5ms/step - loss: 3.6801 - acc: 0.3008\n",
      "Epoch 37/50\n",
      "377669/377669 [==============================] - 2034s 5ms/step - loss: 3.6516 - acc: 0.3051\n",
      "Epoch 38/50\n",
      "377669/377669 [==============================] - 1939s 5ms/step - loss: 3.6235 - acc: 0.3092\n",
      "Epoch 39/50\n",
      "377669/377669 [==============================] - 1921s 5ms/step - loss: 3.5948 - acc: 0.3135\n",
      "Epoch 40/50\n",
      "377669/377669 [==============================] - 1953s 5ms/step - loss: 3.5703 - acc: 0.3179\n",
      "Epoch 41/50\n",
      "377669/377669 [==============================] - 1969s 5ms/step - loss: 3.5412 - acc: 0.3218\n",
      "Epoch 42/50\n",
      "377669/377669 [==============================] - 1977s 5ms/step - loss: 3.5180 - acc: 0.3248\n",
      "Epoch 43/50\n",
      "377669/377669 [==============================] - 1973s 5ms/step - loss: 3.4939 - acc: 0.3288\n",
      "Epoch 44/50\n",
      "377669/377669 [==============================] - 1982s 5ms/step - loss: 3.4730 - acc: 0.3322\n",
      "Epoch 45/50\n",
      "377669/377669 [==============================] - 1986s 5ms/step - loss: 3.4472 - acc: 0.3359\n",
      "Epoch 46/50\n",
      "377669/377669 [==============================] - 1994s 5ms/step - loss: 3.4282 - acc: 0.3393\n",
      "Epoch 47/50\n",
      "377669/377669 [==============================] - 1998s 5ms/step - loss: 3.4071 - acc: 0.3427\n",
      "Epoch 48/50\n",
      "377669/377669 [==============================] - 2004s 5ms/step - loss: 3.3860 - acc: 0.3454\n",
      "Epoch 49/50\n",
      "377669/377669 [==============================] - 2010s 5ms/step - loss: 3.3635 - acc: 0.3493\n",
      "Epoch 50/50\n",
      "377669/377669 [==============================] - 2031s 5ms/step - loss: 3.3468 - acc: 0.3518\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.635801353823869,\n",
       " 6.196471450321056,\n",
       " 5.985404256616327,\n",
       " 5.794297040699274,\n",
       " 5.636348066262054,\n",
       " 5.495194495491133,\n",
       " 5.365041641392477,\n",
       " 5.246202683206313,\n",
       " 5.135976611425109,\n",
       " 5.033620675393083,\n",
       " 4.940721758634721,\n",
       " 4.8534282149747465,\n",
       " 4.772793715482376,\n",
       " 4.696389320866871,\n",
       " 4.625212612327951,\n",
       " 4.5576104618990545,\n",
       " 4.493135081520334,\n",
       " 4.433798131732176,\n",
       " 4.376111340149863,\n",
       " 4.322651716737397,\n",
       " 4.267590020414747,\n",
       " 4.2194214504550285,\n",
       " 4.170011889897325,\n",
       " 4.123761921258943,\n",
       " 4.078591098672879,\n",
       " 4.037202214991048,\n",
       " 3.9961755059099233,\n",
       " 3.954676456271928,\n",
       " 3.917319390838222,\n",
       " 3.8779295952268984,\n",
       " 3.8431745030067006,\n",
       " 3.8086116858718464,\n",
       " 3.7746753046804886,\n",
       " 3.742124948902355,\n",
       " 3.711234480120419,\n",
       " 3.680089154471024,\n",
       " 3.651590141654338,\n",
       " 3.623540153292416,\n",
       " 3.5948442476502267,\n",
       " 3.5703467443689205,\n",
       " 3.541198598717311,\n",
       " 3.518026604217989,\n",
       " 3.4939481144359936,\n",
       " 3.473044967624474,\n",
       " 3.4472151802127096,\n",
       " 3.4282025389923096,\n",
       " 3.407088859222254,\n",
       " 3.3859702372682334,\n",
       " 3.36351086447487,\n",
       " 3.346805119130704]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11bee41198>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/miniconda3/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bed4ae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/eminem-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = \"sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = SEQ_LENGTH - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it was a toy but know im a bit more older i see what it is its like a fucking pair on conjoint simese twins twinnnns one day my mother said to me why dont you ever play outside i used to crawl under my pool and hide i used to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "seed_text = lines[randint(0, len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts, word_to_index):\n",
    "    indices = np.zeros((1, len(texts)), dtype=int)\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        indices[:, i] = word_to_index[text]\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pad_sequences(seq, maxlen):\n",
    "    start = seq.shape[1] - maxlen\n",
    "    \n",
    "    return seq[:, start: start + maxlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, word_to_index, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    \n",
    "    for _ in range(n_words):\n",
    "        encoded = texts_to_sequences(in_text.split(), word_to_index)\n",
    "        encoded = my_pad_sequences(encoded, maxlen=seq_length)\n",
    "        \n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        out_word = ''\n",
    "        \n",
    "        for word, index in word_to_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "                \n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get the back to say the back pounder when you get your ass like a gold nugget what they know about it i can't be a little bit i got places to stop crack it'll make your best side of us just a little person so i don't wanna get\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "generated0 = generate_seq(model, word_to_index, seq_length, seed_text, 50)\n",
    "print(generated0)\n",
    "print(len(generated0.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stampeded\n"
     ]
    }
   ],
   "source": [
    "encoded = texts_to_sequences(generated0.split(), word_to_index)\n",
    "new_encoded = my_pad_sequences(encoded, seq_length)\n",
    "yhat = model.predict_classes(new_encoded, verbose=0)\n",
    "print(index_to_word[yhat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"stampeded and lose mama else to do i did is to begin it and what you shitted on a little stressed away from this shit is none of my body collapse chicka who mighty vodka you don't give a fuck about fun and the fuck i had a little slide\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_seq(model, word_to_index, seq_length, generated0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
